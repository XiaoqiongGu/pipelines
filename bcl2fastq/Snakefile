import os
from math import ceil
from snakemake.utils import report
import getpass

from elmlogger import ElmLogging, UnitId, timestamp


RESULT_OUTDIR = './out'

SAMPLESHEET = os.path.join(RESULT_OUTDIR, 'samplesheet.csv')

USEBASES_CFG = os.path.join(RESULT_OUTDIR, 'usesbases.yaml')


# non-login bash
shell.executable("/bin/bash")
shell.prefix("source snakemake_env.rc;")


def getuser():
    return getpass.getuser()


def bcl2fastq_threads_setting(num_threads):
    """Divide threads meaningfully to the different bcl2fastq 
    """
    demultiplex_threads_frac = 0.2
    process_threads_frac = 0.8
    return " -r 1 -w 1 -d {} -p {}".format(
    	   ceil(demultiplex_threads_frac*num_threads),
	   ceil(process_threads_frac*num_threads))


def get_mux_dirs():
    """returns mux_dir per unit listed in config"""
    return [v['mux_dir'] for k, v in config["units"].items()]


onstart:# available as patched snakemake 3.5.5
    global elm_logger

    if config['ELM']['run_id'] or config['ELM']['library_id'] or config['ELM']['lane_id']:
        unit_ids = [UnitId._make(x) for x in zip(
            config['ELM']['run_id'], config['ELM']['library_id'], config['ELM']['lane_id'])]
    else:
        unit_ids = [UnitId._make(['NA', 'NA', 'NA'])]
        
    elm_logger = ElmLogging(workflow.snakefile,
                            config['ELM']['pipeline_name'],
                            config['ELM']['pipeline_version'],
                            getuser(),#SET_ON_EXEC
                            config['ELM']['site'],
                            timestamp(),# crutch: master jid would be best, but site dependent. log location unknown. how to treat manual runs?
                            config['ELM']['log_path'],#SET_ON_EXEC
                            RESULT_OUTDIR,
                            unit_ids)
    elm_logger.start()
    sys.stderr.write("onstart: FIXME mongodb initiated\n")
    sys.stderr.write("WARN/FIXME what if MUX across several lines? lane_ids needs to be list in muxunits\n")

onsuccess:
    elm_logger.stop(True)
    sys.stderr.write("onsuccess: FIXME mongodb update (success)\n")
onerror:
    elm_logger.stop(False)
    sys.stderr.write("onerror: FIXME mongodb update (fail)\n")


rule final:
    input:
	# here, expand creates a list of expected output folders/files based on 'units'
	# defined in config (actually Project_Y/Sample_X)
        #
        # dependency per mux on bcl2fastq, so that it runs per mux
        expand(os.path.join(RESULT_OUTDIR, '{muxdir}', 'bcl2fastq.SUCCESS'),
               muxdir=get_mux_dirs()),
        expand(os.path.join(RESULT_OUTDIR, '{muxdir}', 'fastqc.SUCCESS'),
               muxdir=get_mux_dirs()),
        expand(os.path.join(RESULT_OUTDIR, '{muxdir}', 'srasubmission.SUCCESS'),
               muxdir=get_mux_dirs()),
        "report.html"
    message:
        """
        Pipeline run successfully completed
        """
    # Set no output in final rule. Otherwise deletion of any input will not result in a rerun


rule report:
    input:
        expand(os.path.join(RESULT_OUTDIR, '{muxdir}', 'bcl2fastq.SUCCESS'),
               muxdir=get_mux_dirs()),

    output: html="report.html"
    run:
        # FIXME duplication with README
        report("""
        ==========================================================================================
        {config[ELM][pipeline_name]} ({config[ELM][pipeline_version]}) report for FIXME:runid
        ==========================================================================================

        FIXME:text        
        """, output.html, metadata="Research Pipeline Development Team (rpd@mailman.gis.a-star.edu.sg)", configfile="conf.yaml")
        # doc "All keywords not listed below are intepreted as paths to files that shall be embedded into the document."
        # **input just attaches all input, but None is not allowed.
        # Attaching configfile is more a crutch
        # FIXME hardcoded path to configfile


rule bcl2fastq:
    """Running bcl2fastq with dynamically split threads

    https://support.illumina.com/content/dam/illumina-support/documents/documentation/software_documentation/bcl2fastq/bcl2fastq2-v2-17-software-guide-15051736-g.pdf
    """
    input: 
        rundir = config['rundir'],
        samplesheet = config['samplesheet_csv'],
    output:
        flag = os.path.join(RESULT_OUTDIR, "{muxdir}", "bcl2fastq.SUCCESS"),
    message: "Running bcl2fastq/Demultiplexing"
    threads: 32
    params:
        usebases = config['usebases_arg'],
        barcode_mismatches = lambda wildcards: '--barcode-mismatches {}'.format(config['units'][wildcards.muxdir]['barcode_mismatches']),
        tiles = lambda wildcards: ' '.join(["--tiles {}".format(lane_id) for lane_id in config['units'][wildcards.muxdir]['lane_id']])
    benchmark: "benchmark/bcl2fastq.txt"
    run:
        cmd = "bcl2fastq --runfolder-dir {input.rundir} --output-dir out --sample-sheet {input.samplesheet} {params.usebases} {params.barcode_mismatches} {params.tiles}" + bcl2fastq_threads_setting(threads)
        #shell(cmd)
        shell('echo {} > {}'.format(cmd, output))
      
       
rule sra_submission:
    """Submitting to SRA"""
    input: '{prefix}/bcl2fastq.SUCCESS'
    output: '{prefix}/srasubmission.SUCCESS'
    message: "Archival submission"
    threads: 1
    benchmark: 'benchmark/sra_submission.txt'
    run:
        sys.stderr.write("FIXME temp hack making sra_submission to appear to work\n")
        for f in output:
            with open(f, 'w') as fh:
                fh.write("FIXME temp hack making sra_submission to appear to work\n")
        return
    
        # Avoid snakemake.exceptions.RuleException: Output files are older than input files
        # which is likely an NFS issue.
        # see also https://bitbucket.org/snakemake/snakemake/issues/300/race-condition-waiting-for-files-on-nfs
        # for now, simply wait/sleep first
        import time
        import os
        import requests
        import json
        
        time.sleep(30)
        for dir in input:
            req = {}
            req_code = {}
            data = { }
            id = dir.split("/")[-2]+"/"+dir.split("/")[-1]
            library_id = config['units'][id]['library_id']
            mux_id = dir.split("/")[-2].split('_')[-1]
            run_id =  config['units'][id]['run_id']
            path = os.path.abspath(dir)
            email = "rpd@mailman.gis.a-star.edu.sg"
            data['library_id'] = library_id
            data['muxId'] = mux_id
            data['run_id'] = run_id
            data['path'] = path
            data['email'] = email
            req_code['reqCode'] = "SA-A002-009"
            req_code['SA-A002-009'] = data
            req['Request'] = req_code
            data_json = json.dumps(req)
            #rest_url =  "http://plap12v:9002/gismart/search/"  ### PRODUCTION
            #rest_url = "http://klap12v:9002/gismart/search/"   ### DEVELOPMENT
            #response = requests.post(rest_url, data=json.dumps(data))
            #if response.status_code != requests.codes.ok:
            #    response.raise_for_status()
            with open("{}/srasubmission.SUCCESS".format(dir), 'w') as fh:
                #fh.write("HDF5_webservice_request_for_archive SRA FIXME\n")
                fh.write(json.dumps(req))
               
rule fastqc:
    """fastqc per muxdir. note: this will not make full use of
    parallelization of many subdirs exist simply because we don't keep
    sample information
    """
    input: '{muxdir}/bcl2fastq.SUCCESS'
    output: '{muxdir}/fastqc.SUCCESS'
    threads: 16
    message: "Running fastqc on {input}"
    benchmark: 'benchmark/fastqc.txt'
    shell:
        # need to be able to deal with empty directories
        # assume success and delete success flag on failure
        "touch {output};"
        "for f in $(find $(dirname {input}) -name \*fastq.gz); do"
        "    fastqc -t {threads} $f || rm {output};"
        "done"





