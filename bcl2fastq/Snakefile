import os
from math import ceil
from snakemake.utils import report
from getpass import getuser
from pipelines import send_status_mail
from elmlogger import ElmLogging, UnitId, timestamp

RESULT_OUTDIR = './out'

SAMPLESHEET = os.path.join(RESULT_OUTDIR, 'samplesheet.csv')
USEBASES_CFG = os.path.join(RESULT_OUTDIR, 'usesbases.yaml')
# ANALYSIS_ID not necessarily part of config file and if not
# needs to be passed to snakemake
assert 'ANALYSIS_ID' in config


# non-login bash
shell.executable("/bin/bash")
shell.prefix("source snakemake_env.rc;")



def bcl2fastq_threads_setting(num_threads):
    """Divide threads meaningfully to the different bcl2fastq 
    """
    demultiplex_threads_frac = 0.2
    process_threads_frac = 0.8
    return " -r 1 -w 1 -d {} -p {}".format(
    	   ceil(demultiplex_threads_frac*num_threads),
	   ceil(process_threads_frac*num_threads))


def get_mux_dirs():
    """returns mux_dir per unit listed in config"""
    return [v['mux_dir'] for k, v in config["units"].items()]

    
def testing_on():
    if 'testing' in config and config['testing']:
        return True
    else:
        return False

    
def mongodb_test_arg():
    if testing_on():
        test_arg="-t"
    else:
        test_arg = ""
    return test_arg

def sra_test_arg():
    if testing_on():
        test_arg="-t"
    else:
        test_arg = ""
    return test_arg
    
    
onstart:# available as patched snakemake 3.5.5
    global elm_logger

    if config['ELM']['run_id'] or config['ELM']['library_id'] or config['ELM']['lane_id']:
        unit_ids = [UnitId._make(x) for x in zip(
            config['ELM']['run_id'], config['ELM']['library_id'], config['ELM']['lane_id'])]
    else:
        unit_ids = [UnitId._make(['NA', 'NA', 'NA'])]
        
    elm_logger = ElmLogging(workflow.snakefile,
                            config['ELM']['pipeline_name'],
                            config['ELM']['pipeline_version'],
                            getuser(),#SET_ON_EXEC
                            config['ELM']['site'],
                            timestamp(),# crutch: master jid would be best, but site dependent. log location unknown. how to treat manual runs?
                            config['ELM']['log_path'],#SET_ON_EXEC
                            RESULT_OUTDIR,
                            unit_ids)
    elm_logger.start()

    
onsuccess:
    elm_logger.stop(True)
    shell("{} -r {} -s SUCCESS -id {} {}".format(
        config['mongo_status'], config['run_num'], conf['ANALYSIS_ID'], mongodb_test_arg()))
    send_status_mail(config['ELM']['pipeline_name'], True, os.path.abspath(RESULT_DIR)) 

    
onerror:
    elm_logger.stop(False)
    shell("{} -r {} -s FAILED -id {} {}".format(
        config['mongo_status'], config['run_num'], conf['ANALYSIS_ID'], mongodb_test_arg()))
    send_status_mail(config['ELM']['pipeline_name'], False, os.path.abspath(RESULT_DIR))

    
rule final:
    input:
	# here, expand creates a list of expected output folders/files based on 'units'
	# defined in config (actually Project_Y/Sample_X)
        #
        # dependency per mux on bcl2fastq, so that it runs per mux
        expand(os.path.join(RESULT_OUTDIR, '{muxdir}', 'bcl2fastq.SUCCESS'),
               muxdir=get_mux_dirs()),
        expand(os.path.join(RESULT_OUTDIR, '{muxdir}', 'fastqc.SUCCESS'),
               muxdir=get_mux_dirs()),
        expand(os.path.join(RESULT_OUTDIR, '{muxdir}', 'srasubmission.SUCCESS'),
               muxdir=get_mux_dirs()),
        "report.html"
    message:
        """
        Pipeline run successfully completed
        """
    # Set no output in final rule. Otherwise deletion of any input will not result in a rerun


rule report:
    input:
        expand(os.path.join(RESULT_OUTDIR, '{muxdir}', 'bcl2fastq.SUCCESS'),
               muxdir=get_mux_dirs()),

    output: html="report.html"
    run:
        # FIXME duplication with README
        report("""
        ==========================================================================================
        {config[ELM][pipeline_name]} ({config[ELM][pipeline_version]}) report for FIXME:runid
        ==========================================================================================

        FIXME:text        
        """, output.html, metadata="Research Pipeline Development Team (rpd@mailman.gis.a-star.edu.sg)", configfile="conf.yaml")
        # doc "All keywords not listed below are intepreted as paths to files that shall be embedded into the document."
        # **input just attaches all input, but None is not allowed.
        # Attaching configfile is more a crutch
        # FIXME hardcoded path to configfile


rule bcl2fastq:
    """Running bcl2fastq with dynamically split threads

    https://support.illumina.com/content/dam/illumina-support/documents/documentation/software_documentation/bcl2fastq/bcl2fastq2-v2-17-software-guide-15051736-g.pdf
    """
    input: 
        rundir = config['rundir'],
        samplesheet = config['samplesheet_csv'],
    output:
        flag = os.path.join(RESULT_OUTDIR, "{muxdir}", "bcl2fastq.SUCCESS"),
    message: "Running bcl2fastq/Demultiplexing"
    threads: 32
    params:
        usebases = config['usebases_arg'],
        barcode_mismatches = lambda wildcards: '--barcode-mismatches {}'.format(config['units'][wildcards.muxdir]['barcode_mismatches']),
        tiles = lambda wildcards: ''.join(["s_{},".format(lane_id) for lane_id in config['units'][wildcards.muxdir]['lane_ids']])[:-1]
        
    benchmark: "benchmark/bcl2fastq.txt"
    run:
        #cmd = "bcl2fastq --runfolder-dir {input.rundir} --output-dir out --sample-sheet {input.samplesheet} {params.usebases} {params.barcode_mismatches} --tiles {params.tiles}" + bcl2fastq_threads_setting(threads)
        #shell(cmd)
        #shell('echo {} > {}'.format(cmd, output))
        cmd = "bcl2fastq --runfolder-dir {input.rundir} --output-dir out --sample-sheet {input.samplesheet} {params.usebases} {params.barcode_mismatches} --tiles {params.tiles}" + bcl2fastq_threads_setting(threads)
        shell(cmd)
        shell("touch {output}")
      
       
rule sra_submission:
    """Submitting to SRA"""
    input: '{prefix}/bcl2fastq.SUCCESS'
    output: '{prefix}/srasubmission.SUCCESS'
    message: "Archival submission"
    threads: 1
    benchmark: 'benchmark/sra_submission.txt'
    run:
        import os
        import sys
        import requests
        import json
        import time
        from getpass import getuser

        if getuser() != "userrig":
            LOG.warn("Not a production user. Archiving skipped")
            with open(output, 'w') as fh:
                fh.write("Not a production user. Archiving skipped\n")
            return

        # Avoid snakemake.exceptions.RuleException: Output files are older than input files
        # which is likely an NFS issue.
        # see also https://bitbucket.org/snakemake/snakemake/issues/300/race-condition-waiting-for-files-on-nfs
        # for now, simply wait/sleep first
        time.sleep(30)
        
        req = {}
        req_code = {}
        project_path = os.path.dirname(f)
        project_id = project_path.split("/")[-1].split('_')[-1]
        run_id =  config['units'][project_path.split("/")[-1]]['run_id']
        email = "veeravallil@.gis.a-star.edu.sg"# FIXME
        success = dict()
        for child in os.listdir(project_path):
            sample_path = os.path.join(project_path, child)
            if os.path.isdir(sample_path):
                data = {}
                abs_sample_path = os.path.abspath(sample_path)
                sample_id = sample_path.split("/")[-1].split('_')[-1]
                data['libraryId'] = sample_id
                data['muxId'] = project_id
                data['runId'] = run_id
                data['path'] = [abs_sample_path]
                data['email'] = [email]
                # FIXME what's this? needs doc
                req_code['reqCode'] = "SA-A002-009"
                req_code['SA-A002-009'] = data
                req['Request'] = req_code
                test_json = json.dumps(req)
                data_json = test_json.replace("\\", "");
                #print(data_json)
                if testing_on():
                    rest_url =  "http://dlap30v:9002/gismart/search"
                else:
                    raise NotImplementedError
                response = requests.post(rest_url, data=data_json)
                #print (response.status_code)
                if response.status_code != requests.codes.ok:
                    success[sample_path] = False
                    LOG.error("Uploading {} failed".format(sample_path))
                    LOG.error("JSON request was {}".format(data_json))
                    LOG.error("Response was {}".format(response.status_code))
                else:
                    success[sample_path] = True
        # FIXME should we fail if any SRA request fails?
        with open(output, 'w') as fh:
            for k, v in success.items():
                fh.write("{} : {}\n".format(k, v))

    
    
rule fastqc:
    """fastqc per muxdir. note: this will not make full use of
    parallelization of many subdirs exist simply because we don't keep
    sample information
    """
    input: '{muxdir}/bcl2fastq.SUCCESS'
    output: '{muxdir}/fastqc.SUCCESS'
    threads: 16
    message: "Running fastqc on {input}"
    benchmark: 'benchmark/fastqc.txt'
    shell:
        # need to be able to deal with empty directories
        # assume success and delete success flag on failure
        # fastqc will fail on corrupted files but return proper error code
        "touch {output};"
        "for f in $(find $(dirname {input}) -name \*fastq.gz); do"
        "    gzip -t $f && fastqc -t {threads} $f || rm {output};"
        "done"





