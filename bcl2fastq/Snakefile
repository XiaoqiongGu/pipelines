# standard library imports
#
import os
from math import ceil
from getpass import getuser

# third party imports
#
import yaml
# only dump() and following do not automatically create aliases
yaml.Dumper.ignore_aliases = lambda *args: True
from snakemake.utils import report

# project specific imports
#
LIB_PATH = os.path.abspath(
    os.path.join(os.path.dirname(os.path.realpath(workflow.snakefile)), "..", "lib"))
if LIB_PATH not in sys.path:
    sys.path.insert(0, LIB_PATH)
from pipelines import send_status_mail
from pipelines import path_to_url
from pipelines import RPD_SIGNATURE
from utils import generate_timestamp
from elmlogger import ElmLogging, ElmUnit
from bcl2fastq_dbupdate import DBUPDATE_TRIGGER_FILE_FMT, DBUPDATE_TRIGGER_FILE_MAXNUM


RESULT_OUTDIR = 'out'


# ANALYSIS_ID not necessarily part of config file and if not
# needs to be passed to snakemake
assert 'ANALYSIS_ID' in config


# non-login bash
shell.executable("/bin/bash")
shell.prefix("source rc/snakemake_env.rc;")


def bcl2fastq_threads_setting(num_threads):
    """Set up threads for the four bcl2fastq stages

    Illumina defaults:
    - 4 threads for reading the data:    -r, --loading-thread
    - 4 threads for writing the data:    -w, --writing-threads
    - 20% for demultiplexing data:       -d, --demultiplexing-threads
    - 100% for processing demultiplexed: -p, --processing-threads
    
    Percent here given as percent of total CPU on system which in our
    case should be the number of threads.

    Processes seem to be mutually exclusive (hard to tell though) and
    IO bound
    """

    r = min([4, num_threads])
    w = min([4, num_threads])
    d = ceil(0.2 * num_threads)
    p = ceil(1.0 * num_threads)
    return "-r {} -w {} -d {} -p {}".format(r, w, d, p)


def get_mux_dirs():
    """returns mux_dir per unit listed in config"""
    return [v['mux_dir'] for k, v in config["units"].items()]

    
    
def write_db_update_trigger(success):
    if success:
        status = 'SUCCESS'
    else:
        status = 'FAILED'
    update_info = {'run_num': config['run_num'],
                   'analysis_id': config['ANALYSIS_ID'],
                   'status': status}
    
    for i in range(DBUPDATE_TRIGGER_FILE_MAXNUM+1):
        dbupdate_trigger_file = DBUPDATE_TRIGGER_FILE_FMT.format(num=i)
        if not os.path.exists(dbupdate_trigger_file):
            break
    assert not os.path.exists(dbupdate_trigger_file)
    with open(dbupdate_trigger_file, 'w') as fh:
        # default_flow_style=None(default)|True(least readable)|False(most readable)
        yaml.dump(update_info, fh, default_flow_style=False)


def barcode_mismatch_arg_for_mux(wildcards):
    if config['units'][wildcards.muxdir]['barcode_mismatches'] is not None:
        arg = '--barcode-mismatches {}'.format(config['units'][wildcards.muxdir]['barcode_mismatches'])
    else:
        arg = ""
    return arg


# NOTE: onstart, onsuccess and onerror are normally in logging.rules
# for analysis pipelines but bcl2fastq needs special versions
onstart:
    global elm_logger

    elm_units = []
    for unit in config['units'].values():
        for lane in unit['lane_ids']:
            eu = ElmUnit._make([unit['run_id'], unit['mux_id'], lane,
                                [os.path.join(RESULT_OUTDIR, unit['mux_dir'])],
                                None])
            elm_units.append(eu)
            
    elm_logger = ElmLogging(workflow.snakefile,
                            config['ELM']['pipeline_name'],
                            config['ELM']['pipeline_version'],
                            getuser(),#SET_ON_EXEC
                            config['ELM']['site'],
                            generate_timestamp(),# crutch: master jid would be best, but impossible to get here
                            config['ELM']['log_path'],#SET_ON_EXEC
                            elm_units)
    elm_logger.start()

    
# NOTE: onstart, onsuccess and onerror are normally in logging.rules
# for analysis pipelines but bcl2fastq needs special versions
onsuccess:
    elm_logger.stop(True)

    extra_text = ""
    for unit in config["units"].values():
        muxname = unit['mux_id']
        muxdir = unit['mux_dir']
        indexhtml = os.path.abspath(os.path.join(
            RESULT_OUTDIR, muxdir, "html/index.html"))
        if os.path.exists(indexhtml):
            try:
                # only works when in production output directory
                url = path_to_url(indexhtml)
            except ValueError:
                # otherwise just report the file path
                url = "file://{}".format(indexhtml)
        else:
            # if missing something's wrong
            url = "ERROR:MISSING"
        extra_text += "Summary for {}: {}\n".format(muxname, url)

    if config.get('mail_on_completion', False):
        send_status_mail(config['ELM']['pipeline_name'], True,
                         "{} ({})".format(config['run_num'], config['ANALYSIS_ID']),
                         os.path.abspath(RESULT_OUTDIR), extra_text=extra_text)
    # cannot talk to mongodb from compute. use trigger file
    write_db_update_trigger(True)

    
onerror:
    elm_logger.stop(False)
    if config.get('mail_on_completion', False):
        send_status_mail(config['ELM']['pipeline_name'], False,
                         "{} ({})".format(config['run_num'], config['ANALYSIS_ID']),
                         os.path.abspath(RESULT_OUTDIR))
    # cannot talk to mongodb from compute. use trigger file
    write_db_update_trigger(False)


localrules: final, report


rule final:
    input:
	# here, expand creates a list of expected output folders/files based on 'units'
	# defined in config (actually Project_Y/Sample_X)
        #
        # dependency per mux on bcl2fastq, so that it runs per mux
        expand(os.path.join(RESULT_OUTDIR, '{muxdir}', 'bcl2fastq.SUCCESS'),
               muxdir=get_mux_dirs()),
        expand(os.path.join(RESULT_OUTDIR, '{muxdir}', 'fastqc.SUCCESS'),
               muxdir=get_mux_dirs()),
        expand(os.path.join(RESULT_OUTDIR, '{muxdir}', 'drop_index_note.SUCCESS'),
               muxdir=get_mux_dirs()),
        report="report.html"
    message:
        """
        Pipeline run successfully completed
        """
    # Set no output in final rule. Otherwise deletion of any input will not result in a rerun


# FIXME almost identical to rules/report.rules but with different analysis_name
rule report:
    input:
        readme = os.path.join(os.path.dirname(os.path.realpath(workflow.snakefile)), "README.md"),
        conf = "conf.yaml",
    output:
        html="report.html"
    params:
        analysis_name = config.get('analysis_name', config['run_num'])
    run:
        report("""
=================================================================
Pipeline {config[ELM][pipeline_name]} run on {params.analysis_name}
=================================================================


- The used pipeline version was {config[ELM][pipeline_version]}
- Parameters including program versions etc. can be found in the attached conf_ file
- Output files can be found in ``./out/``
- The main log file is `./logs/snakemake.log`
- See {input.readme} for a description of this pipeline
""",
               output.html,
               conf=input.conf,
               metadata="Research Pipeline Development Team (rpd@gis.a-star.edu.sg)",
               )
        # from doc "All keywords not listed below are intepreted as paths to files that shall be embedded into the document."
        # **input just attaches all input, but None is not allowed.
        # Attaching configfile is more a crutch to have at least something



rule bcl2fastq:
    """Running bcl2fastq with dynamically split threads

    https://support.illumina.com/content/dam/illumina-support/documents/documentation/software_documentation/bcl2fastq/bcl2fastq2-v2-17-software-guide-15051736-g.pdf
    """
    input: 
        samplesheet = config['samplesheet_csv'],
    output:
        flag = os.path.join(RESULT_OUTDIR, "{muxdir}", "bcl2fastq.SUCCESS"),
    log:
        os.path.join(RESULT_OUTDIR, "{muxdir}.log")
    message:
        "Running bcl2fastq/Demultiplexing"
    threads: 16
    params:
        usebases = config['usebases_arg'],
        barcode_mismatches = barcode_mismatch_arg_for_mux,
        tiles = lambda wildcards: ''.join(["s_{},".format(lane_id) for lane_id in config['units'][wildcards.muxdir]['lane_ids']])[:-1]
    run:
        res_dir = os.path.dirname(output.flag)
        cmd = "bcl2fastq --runfolder-dir {config[rundir]} --output-dir %s" % RESULT_OUTDIR
        cmd += " --stats-dir %s --reports-dir %s" % (res_dir, res_dir)
        cmd += " --create-fastq-for-index-reads"
        cmd += " --sample-sheet {input.samplesheet} {params.usebases}"
        cmd += " {params.barcode_mismatches} --tiles {params.tiles}"
        cmd += " %s" % bcl2fastq_threads_setting(threads)
        cmd += " >& {log}"
        shell(cmd)
        shell("touch {output.flag}")
          

rule fastqc:
    """fastqc per muxdir. note: this will not make full use of
    parallelization of many subdirs exist simply because we don't keep
    sample information
    """
    input: 
        '{muxdir}/bcl2fastq.SUCCESS'
    output:
        touch('{muxdir}/fastqc.SUCCESS')
    log:
        '{muxdir}/fastqc.log'
    threads: 8
    message:
        "Running fastqc on {input}"
    shell:
        # note: need to be able to deal with empty directories.
        # fastqc will fail on corrupted files but return proper error code,
        # so better check input with gzip -t first.
	# rarely saw fastqc threads actually get more than 100% so no point in using threading option
        "{{"
        " find $(dirname {input}) -name \*fastq.gz | xargs --no-run-if-empty -n 1 -P {threads} gzip -t;"
        " find $(dirname {input}) -name \*fastq.gz | xargs --no-run-if-empty -n 1 -P {threads} fastqc;"
        " }} >& {log}"

localrules: drop_index_note
rule drop_index_note:
    """drop a note per index file pointing out that they are...index files.
    """
    input: 
        '{muxdir}/bcl2fastq.SUCCESS'
    output:
        touch('{muxdir}/drop_index_note.SUCCESS')
    log:
        '{muxdir}/drop_index_note.log'
    threads: 8
    message:
        "Running fastqc on {input}"
    run:
        # note: need to be able to deal with empty directories.
        import glob
        for ifq in glob.glob(os.path.join(os.path.dirname(str(input[0])), "*", "*_I[12]_*fastq.gz")):
            with open(ifq + ".README", 'w') as fh:
                fh.write("The _I1_ and _I2_ fastq files are index files.\n")
                fh.write("Use them only if you know exactly what you are doing.\n")
                fh.write("Most users will only be interested in the _R1_ and _R2_ fastq files\n")
                fh.write("\n{}\n".format(RPD_SIGNATURE))
    




